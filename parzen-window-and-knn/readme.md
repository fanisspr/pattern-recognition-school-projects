# Parzen window and Knn estimators
The pdf of a random variable is given by: 
p(x)= 0.5, 0<x<2 and 0 otherwise

The pdf is estimated using the Parzen window and Knn estimators.
- Parzen window estimate uses a gaussian kernel, bandwidth of 0.05 and 0.2, 
  and is plotted using 32, 256 and 5000 points generated by a random number generator according to p(x)  
- Knn estimate uses 32, 64, and 256 neighbors and is plotted using 5000 points.

## Results
Looking at the plots:
- The more datapoints the better the estimate will be. 
- The bigger the h parameter (banwidth), the smoother the pdf
- The bigger the k parameter (neighbors), the smoother the pdf

# Parzen window and Knn classifiers
There are 3 classes having: P(x|ω1) = N(2, 0.5), P(x|ω2) = N(1,1), P(x|ω3) = N(3, 1.2)
and priors: P(ω1) = 0.5, P(ω2) = 0.3, P(ω3) = 0.2.

The following tasks are being executed:
- Create random train sample with 100 samples(features) based on above distributions and priors.
- Create random test sample with 1000 features based on above distributions and priors
- Classify test samples using knn algorithm, for k=1,2,3, and calculate error.
- Compare error to that of the Bayesian classifier.
- Use samples for classification using Parzen Windows. 
- Use atleast 4 different values for the bandwidth parameter h = σ (spread) 
  and choose the one with the best results
- Use samples for classification with Parzen Windows/Probabilistic Neural Networks.
- Use 4 different values for the bandwidth parameter h = σ (spread) 

## Results
KNN algorithms are simple, efficient and accurate. Increasing k neigbours, their accuracy can be increased, but up to a point. 
K should be big enough, so that the error is lessened. Too small and it leads to fuzzy decisions. 
K should also be so big, as to include only the closer samples. Too high and it will lead to excessive smoothing.

We can find the best k, by trying values for k and choosing the one with the best accuracy. 
In our case, k = 10 gives the highest accuracy.

Trying different values for h, we can see that the bigger it is, the smoother the estimate,
 while the smaller the more noisy the estimate.

 We can find the best h, by trying values for h and choosing the one with the best accuracy. 
 In our case, h = 0.31 gives the best accuracy.

We can see that in our case, the bayesian classifier is more accurate compared to the other classifiers.
Knn comes close in second, while parzen window follow with parzen/PNN giving the worst results.

KNN classifiers:

    1-nn accuracy:  0.553
    2-nn accuracy:  0.633
    3-nn accuracy:  0.661
    10-nn accuracy:  0.741

Bayesian classifier:

    Bayesian accuracy: 0.744

Parzen window classifiers:

    Parzen window accuracy, for h= 0.02:  0.604
    Parzen window accuracy, for h= 0.1:  0.682
    Parzen window accuracy, for h= 0.5:  0.695
    Parzen window accuracy, for h= 1:  0.629
    Parzen window accuracy, for h= 4:  0.5
    Parzen window accuracy, for h= 0.31:  0.711

Parzen/PNN classifiers:

    Parzen/Pnn accuracy, for h= 0.02:  0.509
    Parzen/Pnn accuracy, for h= 0.1:  0.549
    Parzen/Pnn accuracy, for h= 0.5:  0.531
    Parzen/Pnn accuracy, for h= 1:  0.506
    Parzen/Pnn accuracy, for h= 4:  0.5